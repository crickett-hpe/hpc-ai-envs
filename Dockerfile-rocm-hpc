ARG BASE_IMAGE
FROM ${BASE_IMAGE}

ARG SCRIPT_DIR=/tmp/dockerfile_scripts
RUN mkdir -p ${SCRIPT_DIR}

# Remove the ompi/ucx, etc that is in the base image
# Seems that the torch installed in the NGC image links against this.
# Wonder if that will cause problems? We can have it use our OMPI but it
# also wants libucs, etc, from UCX. The NGC container must be building
# torch from source and enabling torch distributed with mpi backend.
#RUN rm -rf /opt/hpcx /usr/local/mpi
RUN apt remove -y openmpi ucx || true
#Let's remove existing /opt/ompi; and, link to our version.
RUN rm -rf /opt/ompi
RUN ln -s /container/hpc /opt/ompi

# Put all HPC related tools we build under /container/hpc so we can
# have a shared include, lib, bin, etc to simplify our paths and build steps.
ARG HPC_DIR=/container/hpc
RUN mkdir -p ${HPC_DIR}/bin && \
    mkdir -p ${HPC_DIR}/lib && \
    mkdir -p ${HPC_DIR}/include && \
    mkdir -p ${HPC_DIR}/share && \
    ln -s ${HPC_DIR}/lib ${HPC_DIR}/lib64 && \
    chmod -R go+rX ${HPC_DIR}
ENV LD_LIBRARY_PATH=$HPC_DIR/lib:$LD_LIBRARY_PATH
ENV PATH=$HPC_DIR/bin:$PATH

# Setup some default env variables. This is for the end user as well
# as tools we will build since we put include files under HPC_DIR.
COPY dockerfile_scripts/setup_sh_env.sh ${SCRIPT_DIR}
RUN ${SCRIPT_DIR}/setup_sh_env.sh

# We run this here even though it might be a repeat from the base image
# to make sure we have the required bits for building NCCL, libcxi, etc.
COPY dockerfile_scripts/install_deb_packages.sh ${SCRIPT_DIR}
RUN ${SCRIPT_DIR}/install_deb_packages.sh

# Should we just use /container as the install dir and put
# everything (ie, ucx/ofi/ompi/mpich) under /container/{bin|lib}
# to clean up these arguments?
# Install Cray CXI headers/lib
COPY dockerfile_scripts/cray-libs.sh ${SCRIPT_DIR}
ARG WITH_OFI
RUN if [ "$WITH_OFI" = "1" ]; then \
    ${SCRIPT_DIR}/cray-libs.sh ; \
    fi

#USING OFI
ARG WITH_MPI=1
ARG WITH_OFI=1
ARG OMPI_WITH_CUDA=0
ARG OMPI_WITH_ROCM=1
COPY dockerfile_scripts/ompi.sh ${SCRIPT_DIR}
RUN if [ "$WITH_MPI" = "1" ]; then ${SCRIPT_DIR}/ompi.sh "$UBUNTU_VERSION" "$WITH_OFI" "$OMPI_WITH_ROCM"; fi

# But, only add them if WITH_MPI
ENV LD_LIBRARY_PATH=/container/hpc/lib:$LD_LIBRARY_PATH

#USING OFI
ENV PATH=/container/hpc/bin:$PATH

# Enable running OMPI as root
ENV OMPI_ALLOW_RUN_AS_ROOT=${WITH_MPI:+1}
ENV OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=${WITH_MPI:+1}

ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rocm/lib:/opt/rocm/hip/lib

RUN pip uninstall -y tb-nightly tensorboardX
COPY dockerfile_scripts/additional-requirements.txt ${SCRIPT_DIR}
RUN pip install -r ${SCRIPT_DIR}/additional-requirements.txt


ENV HSA_FORCE_FINE_GRAIN_PCIE=1

ARG AWS_PLUGIN_INSTALL_DIR=/container/hpc
ARG WITH_AWS_TRACE
COPY dockerfile_scripts/build_aws.sh ${SCRIPT_DIR}
RUN if [ "$WITH_OFI" = "1" ]; then ${SCRIPT_DIR}/build_aws.sh "$WITH_OFI" "$WITH_AWS_TRACE"; fi
RUN ldconfig

ARG WITH_NFS_WORKAROUND=1
ENV WITH_NFS_WORKAROUND=$WITH_NFS_WORKAROUND

#MIOPEN_DEBUG_SAVE_TEMP_DIR is required to prevent 
# PAD-133
ENV MIOPEN_DEBUG_SAVE_TEMP_DIR=1

COPY dockerfile_scripts/build_tests.sh ${SCRIPT_DIR}
RUN if [ "$WITH_MPI" = "1" ]; then \
    ${SCRIPT_DIR}/build_tests.sh;  \
    fi


# Set an entrypoint that can scrape up the host libfabric.so and then
# run the user command. This is intended to enable performant execution
# on non-IB systems that have a proprietary libfabric.
COPY dockerfile_scripts/scrape_libs.sh ${SCRIPT_DIR}
RUN mkdir -p /container/bin && \
    cp ${SCRIPT_DIR}/scrape_libs.sh /container/bin
ENTRYPOINT ["/container/bin/scrape_libs.sh"]

CMD ["/bin/bash"]
USER root

RUN rm -r /tmp/*
