ARG BASE_IMAGE
FROM ${BASE_IMAGE}

ARG SCRIPT_DIR=/tmp/dockerfile_scripts/
RUN mkdir -p ${SCRIPT_DIR}

# Remove the ompi/ucx, etc that is in the base image
# Seems that the torch installed in the NGC image links against this.
# Wonder if that will cause problems? We can have it use our OMPI but it
# also wants libucs, etc, from UCX. The NGC container must be building
# torch from source and enabling torch distributed with mpi backend.
#RUN rm -rf /opt/hpcx /usr/local/mpi

# Put all HPC related tools we build under /container/hpc so we can
# have a shared include, lib, bin, etc to simplify our paths and build steps.

COPY dockerfile_scripts/*.sh ${SCRIPT_DIR}
COPY tests/* ${SCRIPT_DIR}

ARG HPC_DIR=/container/hpc
RUN mkdir -p ${HPC_DIR}/bin && \
    mkdir -p ${HPC_DIR}/lib && \
    mkdir -p ${HPC_DIR}/include && \
    mkdir -p ${HPC_DIR}/share && \
    ln -s ${HPC_DIR}/lib ${HPC_DIR}/lib64 && \
    chmod -R go+rX ${HPC_DIR} && \
    ${SCRIPT_DIR}/setup_sh_env.sh && \
    ${SCRIPT_DIR}/install_deb_packages.sh
ENV LD_LIBRARY_PATH=$HPC_DIR/lib:$LD_LIBRARY_PATH
ENV PATH=$HPC_DIR/bin:$PATH

## Setup some default env variables. This is for the end user as well
## as tools we will build since we put include files under HPC_DIR.
#COPY dockerfile_scripts/setup_sh_env.sh ${SCRIPT_DIR}
#RUN ${SCRIPT_DIR}/setup_sh_env.sh

## We run this here even though it might be a repeat from the base image
## to make sure we have the required bits for building NCCL, libcxi, etc.
#COPY dockerfile_scripts/install_deb_packages.sh ${SCRIPT_DIR}
#RUN ${SCRIPT_DIR}/install_deb_packages.sh

ARG WITH_NCCL
ARG WITH_OFI
ARG LIBFABRIC_VERSION
ARG WITH_MPI
ARG OMPI_WITH_CUDA=1
ARG WITH_AWS_TRACE
ARG WITH_XCCL

# If we override NCCL we need to set these env vars for Horovod so that
# it links against the right one later on when we build it.
ENV HOROVOD_NCCL_HOME=${WITH_NCCL:+$HPC_DIR}
ENV HOROVOD_NCCL_LINK=${WITH_NCCL:+SHARED}

ENV OMPI_ALLOW_RUN_AS_ROOT ${WITH_MPI:+1}
ENV OMPI_ALLOW_RUN_AS_ROOT_CONFIRM ${WITH_MPI:+1}
# Need to override this so we don't try using the OMPI built into the
# base container, which is not built correctly for libfabric
ENV OPAL_PREFIX=${WITH_MPI:+$HPC_DIR}

RUN ${SCRIPT_DIR}/build_nccl.sh && \
    ${SCRIPT_DIR}/cray-libs.sh "$LIBFABRIC_VERSION" && \
    ${SCRIPT_DIR}/ompi.sh "$UBUNTU_VERSION" "$WITH_OFI" "$OMPI_WITH_CUDA" && \
    ${SCRIPT_DIR}/build_aws.sh "$WITH_OFI" "$WITH_XCCL" "$WITH_AWS_TRACE" && \
    rm -rf /usr/local/mpi && ln -s /container/hpc /usr/local/mpi && \
    ${SCRIPT_DIR}/build_tests.sh && \
    mkdir -p /container/bin && \
    cp ${SCRIPT_DIR}/scrape_libs.sh /container/bin

ENV NCCL_LIB_DIR=${HOROVOD_NCCL_HOME}/lib
ENV LD_LIBRARY_PATH=${WITH_NCCL:+$NCCL_LIB_DIR:}$LD_LIBRARY_PATH

ENTRYPOINT ["/container/bin/scrape_libs.sh"]
RUN rm -r /tmp/*
